{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "PC-Image2Image_Translation_Pix2Pix.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yogdziex7Gf7"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf_HUOab7Gf8"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from math import log10, sqrt\n",
        "\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import ntpath\n",
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "from scipy.io import wavfile\n",
        "import csv\n",
        "from google.colab import files\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "!pip install pesq\n",
        "from pesq import pesq\n",
        "\n",
        "!pip install -q -U tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVCL09n1K2Gq"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TkMNqDNDodu"
      },
      "source": [
        "# let's try with MOBIPHONE dataset\n",
        "!wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1MflXkdaeAFyBftKkCTTuUXkfJ-fOTzus\" -O \"MOBIPHONE.zip\"\n",
        "!unzip MOBIPHONE.zip\n",
        "PATH = 'MOBIPHONE/'\n",
        "folders = [\"Apple iPhone 5\", \"HTC desire c\", \"HTC Sensation xe\", \"LG GS290\", \"LG L3\", \"LG Optimus L5\", \"LG Optimus L9\", \"Nokia 5530\", \"Nokia C5\", \"Nokia N70\", \"Samsung e1230\", \"Samsung E2121B\", \"Samsung E2600\", \"Samsung Galaxy GT-I9100 s2\", \"Samsung GT-I8190 mini\", \"Samsung GT-N7100 (galaxy note2)\", \"Samsung Galaxy Nexus S\", \"Samsung s5830i\", \"Sony Ericson c902\", \"Sony ericson c510i\"]\n",
        "# \"Vodafone joy 845\" will be used for test, all of the above ones for training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmBCBChKLLRo"
      },
      "source": [
        "# Splitting Dataset into Training and Testing parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsFDWgov4NG"
      },
      "source": [
        "# now, let's convert audio files into images...\n",
        "!mkdir frequency_images_train # directory where to save frequency representation images for train\n",
        "!mkdir frequency_images_test # directory where to save frequency representation images for test\n",
        "\n",
        "# ask input parameters\n",
        "data = input(\"1-STFT / 2-LOG MEL SPECT\\n\")\n",
        "reconstruction = input(\"1-Original Phase / 2-Griffinlim\\n\")\n",
        "while (data != \"1\" and data != \"2\") or (reconstruction != \"1\" and reconstruction != \"2\"):\n",
        "    data = input(\"1-STFT / 2-LOG MEL SPECT\\n\")\n",
        "    reconstruction = input(\"1-Original Phase / 2-Griffinlim\\n\")\n",
        "\n",
        "\n",
        "n_files = 24 # inside test folder\n",
        "i = 1 # index of output file (progressive)\n",
        "starting_point=200 # where to start mask\n",
        "m = 8 # to have a total degradation of 24 rows/columns\n",
        "\n",
        "\n",
        "#training folders cycle\n",
        "id=1 #progressive ID, just to rename images\n",
        "for i in range(len(folders)-1):\n",
        "  for audio in os.listdir(PATH+folders[i]+\"/\"):\n",
        "    ext = os.path.splitext(audio)[-1].lower()\n",
        "    if ext == \".wav\":\n",
        "      y, sr = librosa.load(PATH+folders[i]+\"/\"+audio, sr=16000)\n",
        "      if len(y)>=20*sr-1:\n",
        "        y = y[0:20*sr]\n",
        "        if data == \"1\":\n",
        "          freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "          ph = np.angle(freq_rep)\n",
        "        else:\n",
        "          freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "          ph = np.angle(freq_rep)\n",
        "          freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "        if data == \"1\":\n",
        "          mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "        else:\n",
        "          mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "        # max and min values (in db) for conversion\n",
        "        max_val = np.max(np.max(mag_db))\n",
        "        min_val = np.min(np.min(mag_db))\n",
        "        # converting into image\n",
        "        freq_rep_img = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "        #create also masked image\n",
        "        mask = np.ones([len(freq_rep_img), len(freq_rep_img[0])])\n",
        "        mask[starting_point:(starting_point+m+1), :] = 0\n",
        "        \n",
        "        corrupted_img = mask * freq_rep_img\n",
        "\n",
        "        composed = np.zeros([len(freq_rep_img), len(freq_rep_img[0])*2])\n",
        "        composed[:,0:len(freq_rep_img[0])] = freq_rep_img\n",
        "        composed[:,len(freq_rep_img[0]):len(freq_rep_img[0])*2] = corrupted_img\n",
        "\n",
        "        composed_image = Image.fromarray(composed.astype(np.uint8))  \n",
        "        composed_image.save(\"frequency_images_train/speaker_train_\"+str(id)+\".png\")\n",
        "        id=id+1\n",
        "\n",
        "\n",
        "#testing folder\n",
        "m = 8 # just to start filling it\n",
        "id=1\n",
        "for audio in sorted(os.listdir(PATH+\"Vodafone joy 845/\")):\n",
        "  ext = os.path.splitext(audio)[-1].lower()\n",
        "  if ext == \".wav\":\n",
        "      y, sr = librosa.load(PATH+\"Vodafone joy 845/\"+audio, sr = 16000)\n",
        "      y = y[0:20*sr]\n",
        "      if data == \"1\":\n",
        "         freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "         ph = np.angle(freq_rep)\n",
        "      else:\n",
        "        freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "        ph = np.angle(freq_rep)\n",
        "        freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "      if data == \"1\":\n",
        "        mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "      else:\n",
        "        mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "      # max and min values (in db) for conversion\n",
        "      max_val = np.max(np.max(mag_db))\n",
        "      min_val = np.min(np.min(mag_db))\n",
        "      # converting into image\n",
        "      freq_rep_img = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "      #create also masked image\n",
        "      mask = np.ones([len(freq_rep_img), len(freq_rep_img[0])])\n",
        "      mask[starting_point:(starting_point+m+1), :] = 0\n",
        "      \n",
        "      corrupted_img = mask * freq_rep_img\n",
        "\n",
        "      composed = np.zeros([len(freq_rep_img), len(freq_rep_img[0])*2])\n",
        "      composed[:,0:len(freq_rep_img[0])] = freq_rep_img\n",
        "      composed[:,len(freq_rep_img[0]):len(freq_rep_img[0])*2] = corrupted_img\n",
        "\n",
        "      composed_image = Image.fromarray(composed.astype(np.uint8))\n",
        "      save_name = audio[:-4] #to eliminate '.wav' from file name  \n",
        "      composed_image.save(\"frequency_images_test/speaker_test_\"+str(id)+\".png\")\n",
        "      id=id+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P60WU-1ELa3y"
      },
      "source": [
        "# Training and Testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJToabWq-yqf"
      },
      "source": [
        "# Params\n",
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAWuUiaF-2ZP"
      },
      "source": [
        "# Load real and input(corrupted) image\n",
        "\n",
        "def load(image_file):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_png(image)\n",
        "  w = tf.shape(image)[1]\n",
        "\n",
        "  w = w // 2\n",
        "  real_image = image[:, :w, :]\n",
        "  input_image = image[:, w:, :]\n",
        "\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOWfrnQ-ACAG"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obl4V2ZNAKeF"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T9MV0hqALsa"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIq-7GTCARr1"
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLQ9_UJ7A04m"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HjDpJhNA4rK"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piHz515xA6uo"
      },
      "source": [
        "# Populating Training and Testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWP7kgS-A9A6"
      },
      "source": [
        "train_dataset = tf.data.Dataset.list_files('frequency_images_train/*.png')\n",
        "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yir10d56A_PY"
      },
      "source": [
        "test_dataset = tf.data.Dataset.list_files('frequency_images_test/*.png', shuffle=False)\n",
        "test_dataset = test_dataset.map(load_image_test,deterministic=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Mu1gZIBFN2"
      },
      "source": [
        "# Build the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmH1F-3BH93"
      },
      "source": [
        "OUTPUT_CHANNELS = 1 # gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_CKHc0By-u"
      },
      "source": [
        "## Downsampling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvisNp4wBh3y"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGInXDbsB3Yu"
      },
      "source": [
        "## Upsampling Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZkXG5y8B5fV"
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqkvD-vCJTH"
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[512, 512, 1])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(32, 4, apply_batchnorm=False),\n",
        "    downsample(64, 4), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "    upsample(32, 4)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ewAk_8CNcq"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TMNO83qMpSR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qARKmxRGzgr"
      },
      "source": [
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jvp4M_VERHQ"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWVQXVNMEbTg"
      },
      "source": [
        "# Build the Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADNN7OPNEdNk"
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[512, 512, 1], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[512, 512, 1], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "  down4 = downsample(512, 4)(down3) # (bs, 16, 16, 512)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12L6FMgtFkxZ"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLu5lxTlFx4J"
      },
      "source": [
        "## Discriminator Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1QHLMrG96E"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  # FILL THE CODE:\n",
        "  # define the GAN discriminator loss remember ideally discriminator should classify\n",
        "  # as 0 fake images and as 1 real images\n",
        "  # HINT: use loss_object and tf.ones_like and tf.zeros_like\n",
        "\n",
        "  # loss related to real images\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  # loss related to fake images\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # FILL THE CODE:\n",
        "  # compute output loss as the sum of real and generated loss\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq78AkQtHJyg"
      },
      "source": [
        "# Define the Optimizers and Checkpoint-saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu3iHB0nHAff"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DdvnIlDHC9y"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCEvKrMHMlv"
      },
      "source": [
        "# Generate Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGqVax0YHD_6"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Pix 2 Pix']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    img_to_print = display_list[i].numpy()\n",
        "    img_to_print = img_to_print.squeeze() # to adapt it for imshow\n",
        "    plt.imshow(np.flip(img_to_print,axis=0))\n",
        "  plt.show()\n",
        "\n",
        "  return display_list[2].numpy(), display_list[1].numpy() #returning the predicted image and the original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbWbpuj3H5mn"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4f1-V7IKet"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQMnvyXIOt0"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    # compute the 2 losses\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKwszdZ7IQYc"
      },
      "source": [
        "Actual training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeMqVkOtIZLT"
      },
      "source": [
        "def fit(train_ds, epochs, test_ds):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    for example_input, example_target in test_ds.take(1):\n",
        "      generate_images(generator, example_input, example_target)\n",
        "    print(\"Epoch: \", epoch)\n",
        "\n",
        "    # Train\n",
        "    for n, (input_image, target) in train_ds.enumerate():\n",
        "      print('.', end='')\n",
        "      if (n+1) % 100 == 0:\n",
        "        print()\n",
        "      train_step(input_image, target, epoch)\n",
        "    print()\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjiT_irZIeHI"
      },
      "source": [
        "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
        "\n",
        "To launch the viewer paste the following into a code-cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovnahmdvIeqy"
      },
      "source": [
        "% load_ext tensorboard\n",
        "% tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XXHQxv3I9VZ"
      },
      "source": [
        "Run the training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyZldPzI-5w"
      },
      "source": [
        "fit(train_dataset, EPOCHS, test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Oqx9vsJzwn"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVnb4z4XK733"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfXhMNBeLAG2"
      },
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0WpJmuLThI"
      },
      "source": [
        "# Run the trained model on a few examples from the test dataset\n",
        "for inp, tar in test_dataset.take(5):\n",
        "  out = generate_images(generator, inp, tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTaDb1I7VsqK"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbQuu216Vudv"
      },
      "source": [
        "def convert(freq_img, ph, data, reconstruction, max_val, min_val, sr):\n",
        "  inpainted_freq_rep_db = ((freq_img-0)/(255-0))*(max_val-min_val)+min_val\n",
        "  inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "\n",
        "  if data == \"1\": #stft\n",
        "    if reconstruction == \"1\": #original phase\n",
        "      #inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "      real = inpainted_freq_rep * np.cos(ph)\n",
        "      imag = inpainted_freq_rep * np.sin(ph)\n",
        "      mod_freq_rep = np.zeros([len(real), len(real[0])], dtype=np.complex_)\n",
        "      # inverse transform (creating a complex stft from modified magnitude and original phase)\n",
        "      for i in range(len(real)):\n",
        "        for j in range(len(real[0])):\n",
        "          mod_freq_rep[i, j] = complex(real[i, j], imag[i, j])\n",
        "          \n",
        "      output = librosa.istft(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "\n",
        "    else: #griffinlim\n",
        "      #inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "      output = librosa.griffinlim(inpainted_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "\n",
        "  else: #log mel\n",
        "    if reconstruction == \"1\": #original phase\n",
        "      #inpainted_freq_rep = librosa.db_to_power(inpainted_freq_rep_db)\n",
        "      inpainted_freq_rep = librosa.feature.inverse.mel_to_stft(inpainted_freq_rep, sr, n_fft=1023, power=1.0)\n",
        "      real = inpainted_freq_rep * np.cos(ph)\n",
        "      imag = inpainted_freq_rep * np.sin(ph)\n",
        "      mod_freq_rep = np.zeros([len(real), len(real[0])], dtype=np.complex_)\n",
        "      # inverse transform (creating a complex stft from modified magnitude and original phase)\n",
        "      for i in range(len(real)):\n",
        "        for j in range(len(real[0])):\n",
        "          mod_freq_rep[i, j] = complex(real[i, j], imag[i, j])\n",
        "      \n",
        "      output = librosa.istft(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "      \n",
        "    else: #griffinlim\n",
        "      #inpainted_freq_rep = librosa.db_to_power(inpainted_freq_rep_db)\n",
        "      mod_freq_rep = librosa.feature.inverse.mel_to_stft(inpainted_freq_rep, sr, n_fft=1023,power=1.0)\n",
        "      output = librosa.griffinlim(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "    \n",
        "  return output\n",
        "\n",
        "  def PSNR(original, compressed):\n",
        "  mse = np.mean((original - compressed) ** 2)\n",
        "  if mse == 0:  # MSE is zero means no noise is present in the signal .\n",
        "  # Therefore PSNR have no importance.\n",
        "    return 100\n",
        "  max_pixel = 255.0\n",
        "  psnr = 20 * log10(max_pixel / sqrt(mse))\n",
        "  return psnr\n",
        "\n",
        "def alternative_SNR(y):\n",
        "\n",
        "  def energy(x):\n",
        "    e = np.sum(x ** 2)\n",
        "    return e\n",
        "\n",
        "  Z = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "  z = librosa.istft(Z, win_length=625, hop_length=625, window='rect')\n",
        "  n = y[:z.size] - z\n",
        "  snr = 10 * np.log10(energy(y)/energy(n))\n",
        "  return snr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yQ5OIdyOg6a"
      },
      "source": [
        "# Main Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhTYD1dyWSP2"
      },
      "source": [
        "iteration = round(m/2)+1\n",
        "n_files = 24\n",
        "file_list = sorted(os.listdir('MOBIPHONE/Vodafone joy 845/'))\n",
        "image_list = sorted(os.listdir('frequency_images_test/'))\n",
        "input_indices = [\"1\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"2\",\"20\",\"21\",\"22\",\"23\",\"24\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "#THE CORRESPONDANCE IS (ex) : speaker_10.png --> speaker18.wav --> output_2.wav and so on...\n",
        "\n",
        "i = 0\n",
        "# delete old csv if present\n",
        "if os.path.isfile('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv'):\n",
        "  os.remove('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv')\n",
        "\n",
        "# creating csv file where to save results (SNR/PSNR)\n",
        "with open('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv', mode='w') as pix_2_pix: # OPENING FILE (TILL THE END)\n",
        "  pix_2_pix_writer = csv.writer(pix_2_pix, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  pix_2_pix_writer.writerow(['FILE', 'SNR', 'PSNR', 'SSIM', 'PESQ'])\n",
        "  #so, for example...model_based_results_1_1.csv means results of data = 1 (stft) and reconstruction = 1 (original phase)\n",
        "  for inp, tar in test_dataset.take(n_files):\n",
        "    print(\"\\nFILE \"+str(i+1)+\"...\")\n",
        "    out, orig = generate_images(generator, inp, tar)\n",
        "        \n",
        "    #recovering phase\n",
        "    index = int(input_indices[i])\n",
        "    audio_to_load = file_list[index-1]\n",
        "    y, sr = librosa.load('MOBIPHONE/Vodafone joy 845/'+audio_to_load, sr = 16000)\n",
        "    y = y[0:20*sr]\n",
        "    if data == \"1\":\n",
        "      freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "      ph = np.angle(freq_rep)\n",
        "    else:\n",
        "      freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "      ph = np.angle(freq_rep)\n",
        "      freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "    # recovering also original image (for PSNR)\n",
        "    if data == \"1\":\n",
        "      mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "    else:\n",
        "      mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "    # max and min values (in db) for conversion\n",
        "    max_val = np.max(np.max(mag_db))\n",
        "    min_val = np.min(np.min(mag_db))\n",
        "    # converting into image\n",
        "    img_np = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "    out_np = out #will be the last inpainted (predicted) image\n",
        "    orig_np = orig #will be the original image\n",
        "\n",
        "    # converting into 0,255\n",
        "    max_val_orig = np.max(np.max(orig_np[:,:,0]))\n",
        "    min_val_orig = np.min(np.min(orig_np[:,:,0]))\n",
        "    max_o = np.max(np.max(out_np[:,:,0]))\n",
        "    min_o = np.min(np.min(out_np[:,:,0]))\n",
        "    # converting into image\n",
        "    orig_np = np.round(((orig_np[:,:,0]-min_val_orig)/(max_val_orig-min_val_orig))*(255-0)+0)\n",
        "    out_np = np.round(((out_np[:,:,0] - min_o) / (max_o - min_o)) * (255 - 0) + 0)\n",
        "      \n",
        "    #PSNR\n",
        "    psnr = PSNR(orig_np[:,:], out_np[:,:])\n",
        "        \n",
        "    min_val_out = np.min(np.min(out_np[:,:]))\n",
        "    max_val_out = np.max(np.max(out_np[:,:]))\n",
        "\n",
        "    output = convert(out_np[:,:], ph, data, reconstruction, max_val, min_val, sr)\n",
        "    min_out = np.min(output)\n",
        "    max_out = np.max(output)\n",
        "    output=((output-min_out)/(max_out-min_out))*(max(y)-min(y))+min(y) # converting back\n",
        "        \n",
        "    # writing output\n",
        "    wavfile.write('output_'+str(i+1)+'.wav', sr, output.astype(np.float32))\n",
        "\n",
        "    # SNR\n",
        "    o, sr = librosa.load('output_'+str(i+1)+'.wav',sr=16000)\n",
        "    snr = alternative_SNR(o)\n",
        "          \n",
        "    #PESQ\n",
        "    psq = pesq(16000, y, o, 'wb')\n",
        "\n",
        "    # SSIM\n",
        "    images_ssim = ssim(orig_np[:,:], out_np[:,:], data_range=orig_np[:,:].max() - out_np[:,:].min())\n",
        "\n",
        "    # save results in csv\n",
        "    pix_2_pix_writer.writerow([os.path.basename(str('MOBIPHONE/Vodafone joy 845/'+str(audio_to_load[:-4]))), str(snr), str(psnr), str(images_ssim), str(psq)])\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "\n",
        "# download csv\n",
        "files.download('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}